# Backpropagation & Neural Networks

We've talked about
- scores function
 - sample loss
	 - SVM
	 - SoftMax
- data loss + regularization

To find the best W, we want the gradient of the loss function on W

---

Numerical gradient
- slow
- approximate
- easy to write

Analytic gradient
- fast
- exact
- error-prone

## Computational Graph
![[Pasted image 20241005225250.png]]
This is an example of the linear classification
1. using the input x and weights W to compute a vector of scores s
2. using hinge loss to compute the sample loss Li
3. adding the regularization term to compute the total loss L

Once we can have a computational graph, we can apply backpropagation which recursively use the chain rule to compute the gradient.

![[Pasted image 20241005230840.png]]
Our goal are the partial derivatives, which represents the gradients of each variable.
![[Pasted image 20241005231703.png]]
![[Pasted image 20241005232638.png]]
Compute the basic gradients first, then we can compute the complex ones using the chain rule.

![[Pasted image 20241005233046.png]]

The green numbers should a flow of the forward propagation.

Backpropagation simplifies the process of partial derivative. What we need to do is just calculate the local gradient, and then multiply by the upstream gradient.

For example,

1. It is definite that $\frac{df}{df}=1$, which is the first upstream.
2. Compute the local gradient
	1. $\frac d{dx}\frac1x=-\frac{1}{x^2}=-\frac{1}{1.37^2}=-0.53$
3. Multiply by the upstream gradient
	1. $-0.53\times 1=-0.53$

Same for the next round,

1. The upstream gradient is $-0.53$
2. Compute the local gradient
	1. $\frac d{dx}1+x=1$
3. Multiply by the upstream gradient
	1. $1\times-0.53=-0.53$

Keep going,

1. The upstream gradient is $-0.53$
2. Compute the local gradient
	1. $\frac d{dx}e^x=e^x=e^{-1}$
3. Multiply by the upstream gradient
	1. $e^{-1}\times-0.53=-0.2$


If the chain has no branches, we can group some nodes and write the analytic gradient so it can be considered as one node.

![[Pasted image 20241006015936.png]]

And we can simplify some of the gates
- Add
	- Just pass the upstream
	- Distributor
- Max
	- Pass it to the one has larger input
		- Because you can consider a max() as a piecewise function
			- The large one is $1*x$
			- The small one is $0*x$
	- Router
- Mul
	- Scale the upstream gradient by another variable
	- Switcher

When inputs are vectors instead of a single scalar, the local gradient should be a Jacobian Matrix because the input and the output are both vectors.
$\frac{dL}{dx}=\frac{df}{dx}\cdot\frac{dL}{df}$
- Derivatives on L are vectors
- Derivative on f is a Jacobian Matrix
	- Diagonal
![[Pasted image 20241006142416.png]]
![[Pasted image 20241006152427.png]]

---
## Modularized Implementation: Forward / Backward API

```python
class MultiplyGate(obj):
	def forward(x,y):
		z = x*y
		self.x = x
		self.y = y
		return z
	def backward(dz):
		dx = self.y * dz
		dy = self.x * dz
		return [dx, dy]
```
