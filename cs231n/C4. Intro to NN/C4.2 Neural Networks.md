# Without the Brain Stuff
- Before
	- Linear score function
		- $f=Wx$
- Now
	- 2-layer neural network
		- $f=W_2max(0,W_1x)$
	- 3-layer neural network
		- $f=W_3max(0,W_2max(0,W_1x))$
	- Deep neural network

Between the two linear functions, there should be a non-linear function. Otherwise, they will collapse into one linear function.

Each layer can be considered as a score system of some features.
We talked about the W can be transformed back into an image which is a template of the class. However, an object in the real world is complex and might not be able to fit in the template we trained. For example, the template of a car might look like a red BMW, but a white Mercedes is also a car. Thus, having multiple layers, that is multiple templates, will help training a good model.

![[Pasted image 20241006165053.png]]
The activation function takes all inputs then ==outputs only one number.==
- Sigmoid
	- $\sigma(x)=\frac1{1+e^{-x}}$
- ReLU
	- $max(0,x)$
- Tanh
	- $tanh(x)$
- Leaky ReLU
	- $max(0.1x,x)$
- Maxout
	- $max(w_1^Tx+b_1,w_2^Tx+b_2)$
- ELU
	- $x\ge0:$ $x$
	- $x<0:$ $\alpha(e^x-1)$


![[Pasted image 20241006170057.png]]

```python
f = lambda x: 1.0/(1.0 + np.exp(-x)) # sigmoid activation
x = np.random.randn(3,1) # 3 by 1
h1 = f(np.dot(W1,x) + b1) # 4 by 1
h2 = f(np.dot(W2,h1) + b2) # 4 by 1
out = np.dot(W3,h2) + b3 # 1 by 1
```

