Linear Classifier is an example of ==parametric classifier== where all of our knowledge of the training data are summarized into the parameter matrix W set by the process of training.

- flatten the pixel file into a column vector
- using W to convert them into scores of classes
- using the class with the highest score as the prediction

# How to Compute W
- define a ==loss function== that ==quantifies== our un happiness with the scores across the training data
- come up with a way of efficiently finding the parameters that ==minimize== the loss function (==optimization==)

$L=\frac1N\sum L_i(f(x_i,W),y_i)$
- the average loss of truth-predict pairs

## Multiclass SVM loss

$L_i=\sum_{j\not=y_i}max(0,s_j+1-s_{y_i})$
- if the score for the correct category is greater than the incorrect score by some safety margin, which is 1 here, then the loss is 0. Otherwise, the loss is the distance between them
- sum the loss of each incorrect categories, we got the loss for this example

![[Pasted image 20240928145640.png]]
- $i$
	- the index of the example in the dataset
- $y_i$
	- the correct category of the i-th example in the dataset
- $j$
	- one of the incorrect categories
- $s_{y_i}$
	- the score of the correct category of the i-th example in the dataset
- $s_j$
	- the score of one of the incorrect categories of the i-th example in the dataset

This is one of the Hinge Loss, which set the loss 0 when passing a safety margin.

![[Pasted image 20240928150744.png]]

# Regularization

Add a penalty so the model would not be too complicate that overfits the training set.

The model has to overcome the penalty if it wants to have high dimension or more parameters. 

$L(W)=\frac1N\sum^N_{i=1}L_i(f(x_i,W),y_i)+\lambda R(W)$

	Occam's Razor
	"Among competing hypotheses, the simplest it the best"
	- William of Ockham, 1285-1347

- $\lambda$
	- a hyperparameter used to trade-off the regularization term and the data loss

- $R(W)$
	- in common use
		- L2
		- L1
		- elastic net
			- L1 + L2
		- max norm regularization
		- dropout
	- fancier
		- batch normalization 
		- stochastic depth

# SoftMax Classifier (Multinomial Logistic Regression)

scores = unnormalized log probabilities of the classes

$P(Y=k|X=x_i)=\frac{e^{s_k}}{\sum_je^{s_j}}$
$s=f(x_i;W)$

==Cross-Entropy Loss==

SoftMax transfers the scores into ==probabilities==

Now we can maximize the log likelihood or minimize the negative log likelihood of the correct class (Loss function)
$L_i=-log P(Y=y_i|X=x_i)$

$L_i = -log(\frac{e^{s_k}}{\sum_je^{s_j}})$
![[Pasted image 20241005155053.png]]

![[Pasted image 20241005200549.png]] 

SVM is trying to reduce the scores of all incorrect classes under the safety margin.
SoftMax is trying to pile up the score of the correct class to one.

When the score of the correct class is very high, SVM will not care about the incorrect classes while SoftMax is not going to stop because the probability of the correct class will never reach one in practice.

SVM only want to know YES or NO.
SoftMax cares about know how great it is.

# Optimization
It is impractical to find the local / global minimum in one step like derivative once your predict function F, regularization term R gets big and complex and applying neural networks.

Instead, we iterate and improve each round.

The direction we choose to go should be the one has the greatest negative slope. That's gradient.

## Gradient Descent

How far we would like to go in that direction
- learning rate
	- the most important hyperparameter

## Stochastic Gradient Descent
Using the whole training set for one update is expensive, we can sample a mini batch to estimate the gradient.

## Image Features: Motivation

![[Pasted image 20241005220143.png]]
In this case, we apply polar coordinate to transform the features, which turns a complex dataset into a simple one that can be linearly separated.

An example is Histogram of Oriented Gradients
![[Pasted image 20241005220623.png]]
HoG divides the image into small regions and then find the dominant edge type. Putting all the dominant edges of each region, we get a feature map.

![[Pasted image 20241005221103.png]]
 