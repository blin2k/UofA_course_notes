1957, Frank Rosenblatt developed the Mark I Perceptron machine.
- The first implementation of the perceptron algorithm

It only produce 1 or 0
- 1 if $wx+b>0$
- 0 otherwise
It looks like ReLU

Update rule
- $w_i(t+1)=w_i(t)+\alpha(d_j-y_j(t))x_{j,i}$
It looks like backprop

---
1960, Widrow and Hoff developed Adaline/Madaline.
- Stacking linear layers into multi-layer perceptron networks
It looks like layers

---
1986, Rumelhart introduced Backpropagation.

---
2006, Hinton and Salakhutdinov showed that we can effectively train a deep neural network.
- Restricted Boltzmann machine
- Pre-train each hidden layer to initialize

---
2012, Geoffrey E Hinton's lab implemented 
- acoustic modeling using deep belief networks
- Context-dependent pre-trained deep neural networks for large vocabulary speech recognition
- ImageNet classification with deep convolutional neural networks.
	- The benchmark of image classification

---
Hubel and Wiesel
![[Pasted image 20241006172909.png]]
How cats process visual stimulus

![[Pasted image 20241006173049.png]]
![[Pasted image 20241006173141.png]] ![[Pasted image 20241006173335.png]]
Taking advantage of the power of GPU.